{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9705ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lpips\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage.color import lab2rgb, rgb2lab\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from colormath.color_conversions import convert_color\n",
    "from colormath.color_diff import delta_e_cie2000\n",
    "from colormath.color_objects import LabColor, sRGBColor\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*negative Z values.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = torch.device(\"cuda\")\n",
    "        name = torch.cuda.get_device_name(0)\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        dev = torch.device(\"mps\")\n",
    "        name = \"Apple Silicon\"\n",
    "    else:\n",
    "        dev = torch.device(\"cpu\")\n",
    "        name = \"CPU\"\n",
    "    return dev, name\n",
    "\n",
    "device, device_name = get_device()\n",
    "print(f\"Using device: {device_name} ({device})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c871b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_ID = 'C'\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 256\n",
    "EPOCHS = 50\n",
    "SAVE_INTERVAL = 10\n",
    "\n",
    "# Model-specific configuration\n",
    "USE_GAN = True\n",
    "USE_L1 = True\n",
    "USE_PERCEPTUAL = True\n",
    "LAMBDA_L1 = 100.0\n",
    "LAMBDA_PERCEPTUAL = 10.0\n",
    "\n",
    "print(f\"Model {MODEL_ID} Configuration:\")\n",
    "print(f\"  GAN: {USE_GAN}\")\n",
    "print(f\"  L1 Loss: {USE_L1} (λ₁ = {LAMBDA_L1})\")\n",
    "print(f\"  Perceptual Loss: {USE_PERCEPTUAL} (λ₂ = {LAMBDA_PERCEPTUAL})\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a20a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ColorizeDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_size=256, split='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        \n",
    "        color_dir = self.root_dir / f\"{split}_color\"\n",
    "        self.color_paths = sorted(glob.glob(str(color_dir / \"*.jpg\")))\n",
    "        \n",
    "        black_dir = self.root_dir / f\"{split}_black\"\n",
    "        self.black_paths = sorted(glob.glob(str(black_dir / \"*.jpg\")))\n",
    "        \n",
    "        assert len(self.color_paths) == len(self.black_paths)\n",
    "        \n",
    "        # Data augmentation for training set\n",
    "        if split == 'train':\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            # No augmentation for validation and test sets\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        \n",
    "        print(f\"Loaded {len(self.color_paths)} {split} image pairs\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.color_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        color_img = Image.open(self.color_paths[idx]).convert('RGB')\n",
    "        color_img = self.transform(color_img)\n",
    "        \n",
    "        gray_img = Image.open(self.black_paths[idx]).convert('L')\n",
    "        gray_img = self.transform(gray_img)\n",
    "        \n",
    "        color_img_np = (color_img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "        lab_img = rgb2lab(color_img_np)\n",
    "        \n",
    "        L = lab_img[:, :, 0] / 50.0 - 1.0\n",
    "        ab = lab_img[:, :, 1:] / 128.0\n",
    "        \n",
    "        L = torch.from_numpy(L).unsqueeze(0).float()\n",
    "        ab = torch.from_numpy(ab).permute(2, 0, 1).float()\n",
    "        \n",
    "        return L, ab, gray_img\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ColorizeDataset('../data/colorize_dataset/data', img_size=IMG_SIZE, split='train')\n",
    "val_dataset = ColorizeDataset('../data/colorize_dataset/data', img_size=IMG_SIZE, split='val')\n",
    "test_dataset = ColorizeDataset('../data/colorize_dataset/data', img_size=IMG_SIZE, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True if torch.cuda.is_available() else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if torch.cuda.is_available() else False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if torch.cuda.is_available() else False)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set:   {len(train_dataset):4d} images ({len(train_loader):3d} batches)\")\n",
    "print(f\"Validation set: {len(val_dataset):4d} images ({len(val_loader):3d} batches)\")\n",
    "print(f\"Test set:       {len(test_dataset):4d} images ({len(test_loader):3d} batches)\")\n",
    "print(f\"Image size:     {IMG_SIZE}×{IMG_SIZE} pixels\")\n",
    "print(f\"Batch size:     {BATCH_SIZE}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3885d",
   "metadata": {},
   "source": [
    "## U-Net Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net components\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down1 = nn.Conv2d(in_channels, 64, 4, 2, 1)\n",
    "        self.down2 = UNetBlock(64, 128, down=True)\n",
    "        self.down3 = UNetBlock(128, 256, down=True)\n",
    "        self.down4 = UNetBlock(256, 512, down=True)\n",
    "        self.down5 = UNetBlock(512, 512, down=True)\n",
    "        self.down6 = UNetBlock(512, 512, down=True)\n",
    "        self.down7 = UNetBlock(512, 512, down=True)\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
    "        self.up2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
    "        self.up3 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
    "        self.up4 = UNetBlock(1024, 512, down=False)\n",
    "        self.up5 = UNetBlock(1024, 256, down=False)\n",
    "        self.up6 = UNetBlock(512, 128, down=False)\n",
    "        self.up7 = UNetBlock(256, 64, down=False)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        \n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        \n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d7], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d6], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d5], dim=1))\n",
    "        u5 = self.up5(torch.cat([u4, d4], dim=1))\n",
    "        u6 = self.up6(torch.cat([u5, d3], dim=1))\n",
    "        u7 = self.up7(torch.cat([u6, d2], dim=1))\n",
    "        \n",
    "        output = self.final(torch.cat([u7, d1], dim=1))\n",
    "        return output\n",
    "\n",
    "# Initialize generator\n",
    "generator = UNetGenerator(in_channels=1, out_channels=2).to(device)\n",
    "print(\"U-Net Generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776b0ed",
   "metadata": {},
   "source": [
    "## PatchGAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatchGAN Discriminator\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    \"\"\"70x70 PatchGAN discriminator for conditional GAN\"\"\"\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, 2, 1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, L, ab):\n",
    "        x = torch.cat([L, ab], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize discriminator\n",
    "discriminator = PatchGANDiscriminator(in_channels=3).to(device)\n",
    "print(\"PatchGAN Discriminator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4c3eb",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e6d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptual Loss using VGG16\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual loss using VGG16 features\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            from torchvision.models import VGG16_Weights\n",
    "            vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
    "        except ImportError:\n",
    "            vgg = models.vgg16(pretrained=True).features\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.children())[:16]).eval()\n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.criterion = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, generated, target):\n",
    "        gen_rgb = torch.cat([generated, generated[:, :1, :, :]], dim=1)\n",
    "        target_rgb = torch.cat([target, target[:, :1, :, :]], dim=1)\n",
    "        \n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(generated.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(generated.device)\n",
    "        \n",
    "        gen_normalized = (gen_rgb - mean) / std\n",
    "        target_normalized = (target_rgb - mean) / std\n",
    "        \n",
    "        gen_features = self.feature_extractor(gen_normalized)\n",
    "        target_features = self.feature_extractor(target_normalized)\n",
    "        \n",
    "        return self.criterion(gen_features, target_features)\n",
    "\n",
    "# GAN Loss\n",
    "class GANLoss(nn.Module):\n",
    "    \"\"\"GAN loss (BCE with logits)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, prediction, target_is_real):\n",
    "        if target_is_real:\n",
    "            target = torch.ones_like(prediction)\n",
    "        else:\n",
    "            target = torch.zeros_like(prediction)\n",
    "        return self.criterion(prediction, target)\n",
    "\n",
    "# Initialize loss functions\n",
    "l1_loss = nn.L1Loss()\n",
    "perceptual_loss = PerceptualLoss().to(device)\n",
    "gan_loss = GANLoss()\n",
    "\n",
    "print(\"All loss functions initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00816246",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [], \n",
    "    'val_loss': [],\n",
    "    'gen_loss': [],\n",
    "    'disc_loss': []\n",
    "}\n",
    "\n",
    "# Create save directories\n",
    "save_dir = Path(f'../models/{MODEL_ID}')\n",
    "results_dir = Path('../results')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Model will be saved to: {save_dir}\")\n",
    "print(f\"Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc20b93",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate_epoch(generator, val_loader, device):\n",
    "    generator.eval()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for L, ab_real, _ in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            L = L.to(device)\n",
    "            ab_real = ab_real.to(device)\n",
    "            \n",
    "            ab_fake = generator(L)\n",
    "            \n",
    "            # Calculate loss: L1 + Perceptual (skip GAN loss for validation)\n",
    "            loss = l1_loss(ab_fake, ab_real) * LAMBDA_L1 + perceptual_loss(ab_fake, ab_real) * LAMBDA_PERCEPTUAL\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(val_loader)\n",
    "\n",
    "# Visualization function for validation samples\n",
    "def visualize_validation_samples(generator, val_loader, device, epoch, save_dir, num_samples=5):\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        L, ab_real, gray_img = next(iter(val_loader))\n",
    "        L = L[:num_samples].to(device)\n",
    "        ab_real = ab_real[:num_samples]\n",
    "        gray_img = gray_img[:num_samples]\n",
    "        \n",
    "        ab_fake = generator(L).cpu()\n",
    "        L = L.cpu()\n",
    "        \n",
    "        fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Grayscale input\n",
    "            axes[i, 0].imshow(gray_img[i].squeeze(), cmap='gray')\n",
    "            axes[i, 0].set_title('Input (Grayscale)', fontweight='bold')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Generated colorization\n",
    "            L_np = ((L[i].squeeze().numpy() + 1.0) * 50.0)\n",
    "            ab_fake_np = ab_fake[i].permute(1, 2, 0).numpy() * 128.0\n",
    "            lab_fake = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "            lab_fake[:, :, 0] = L_np\n",
    "            lab_fake[:, :, 1:] = ab_fake_np\n",
    "            rgb_fake = lab2rgb(lab_fake)\n",
    "            \n",
    "            axes[i, 1].imshow(rgb_fake)\n",
    "            axes[i, 1].set_title('Generated Color', fontweight='bold')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Ground truth\n",
    "            ab_real_np = ab_real[i].permute(1, 2, 0).numpy() * 128.0\n",
    "            lab_real = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "            lab_real[:, :, 0] = L_np\n",
    "            lab_real[:, :, 1:] = ab_real_np\n",
    "            rgb_real = lab2rgb(lab_real)\n",
    "            \n",
    "            axes[i, 2].imshow(rgb_real)\n",
    "            axes[i, 2].set_title('Ground Truth', fontweight='bold')\n",
    "            axes[i, 2].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Validation Samples - Epoch {epoch}', fontsize=16, fontweight='bold', y=1.0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_path = save_dir / f'val_samples_epoch_{epoch}.png'\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  ✓ Validation samples saved to: {output_path.name}\")\n",
    "        \n",
    "        # Display the plot in notebook\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(generator, discriminator, train_loader, gen_optimizer, disc_optimizer, epoch, device):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    epoch_gen_loss = 0.0\n",
    "    epoch_disc_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for L, ab_real, _ in pbar:\n",
    "        L = L.to(device)\n",
    "        ab_real = ab_real.to(device)\n",
    "        \n",
    "        # ==================\n",
    "        # Train Discriminator\n",
    "        # ==================\n",
    "        disc_optimizer.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        ab_fake = generator(L)\n",
    "        \n",
    "        # Discriminator on real images\n",
    "        pred_real = discriminator(L, ab_real)\n",
    "        loss_real = gan_loss(pred_real, True)\n",
    "        \n",
    "        # Discriminator on fake images\n",
    "        pred_fake = discriminator(L, ab_fake.detach())\n",
    "        loss_fake = gan_loss(pred_fake, False)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        loss_disc = (loss_real + loss_fake) * 0.5\n",
    "        loss_disc.backward()\n",
    "        disc_optimizer.step()\n",
    "        \n",
    "        # ================\n",
    "        # Train Generator\n",
    "        # ================\n",
    "        gen_optimizer.zero_grad()\n",
    "        \n",
    "        # Generator wants discriminator to classify fake as real\n",
    "        pred_fake = discriminator(L, ab_fake)\n",
    "        loss_gan = gan_loss(pred_fake, True)\n",
    "        \n",
    "        # L1 loss\n",
    "        loss_l1 = l1_loss(ab_fake, ab_real) * LAMBDA_L1\n",
    "        \n",
    "        # Perceptual loss\n",
    "        loss_perceptual_val = perceptual_loss(ab_fake, ab_real) * LAMBDA_PERCEPTUAL\n",
    "        \n",
    "        # Total generator loss\n",
    "        loss_gen = loss_gan + loss_l1 + loss_perceptual_val\n",
    "        loss_gen.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        epoch_gen_loss += loss_gen.item()\n",
    "        epoch_disc_loss += loss_disc.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'G_loss': f\"{loss_gen.item():.4f}\",\n",
    "            'D_loss': f\"{loss_disc.item():.4f}\"\n",
    "        })\n",
    "    \n",
    "    return epoch_gen_loss / len(train_loader), epoch_disc_loss / len(train_loader)\n",
    "\n",
    "# Train model\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Training - Model {MODEL_ID}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train\n",
    "    avg_gen_loss, avg_disc_loss = train_epoch(\n",
    "        generator, discriminator, train_loader, \n",
    "        gen_optimizer, disc_optimizer, epoch, device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    avg_val_loss = validate_epoch(generator, val_loader, device)\n",
    "    \n",
    "    # Update history\n",
    "    history['epoch'].append(epoch)\n",
    "    history['gen_loss'].append(avg_gen_loss)\n",
    "    history['disc_loss'].append(avg_disc_loss)\n",
    "    history['train_loss'].append(avg_gen_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"  Generator Loss: {avg_gen_loss:.4f}\")\n",
    "    print(f\"  Discriminator Loss: {avg_disc_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n",
    "            'disc_optimizer_state_dict': disc_optimizer.state_dict(),\n",
    "            'history': history,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_train_loss': avg_gen_loss\n",
    "        }\n",
    "        torch.save(checkpoint, save_dir / 'best_model.pt')\n",
    "        print(f\"  ✓ New best model saved! (Val Loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Save checkpoint at intervals\n",
    "    if epoch % SAVE_INTERVAL == 0:\n",
    "        checkpoint_path = save_dir / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"  ✓ Checkpoint saved at epoch {epoch}\")\n",
    "        \n",
    "        # Visualize validation samples every 10 epochs\n",
    "        visualize_validation_samples(generator, val_loader, device, epoch, results_dir)\n",
    "    \n",
    "    # Save training history to CSV after each epoch\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_csv_path = results_dir / f'training_history_{MODEL_ID}.csv'\n",
    "    history_df.to_csv(history_csv_path, index=False)\n",
    "    \n",
    "    # Clear cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Training history saved to: {history_csv_path}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07779e2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch numpy for colormath compatibility\n",
    "if not hasattr(np, 'asscalar'):\n",
    "    np.asscalar = lambda x: x.item() if hasattr(x, 'item') else float(x)\n",
    "\n",
    "# Evaluation metrics class\n",
    "class ColorimetricEvaluator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "        self.lpips_model.eval()\n",
    "    \n",
    "    def calculate_psnr(self, generated, target):\n",
    "        gen_np = generated.cpu().numpy()\n",
    "        target_np = target.cpu().numpy()\n",
    "        psnr_values = []\n",
    "        for i in range(gen_np.shape[0]):\n",
    "            psnr = peak_signal_noise_ratio(target_np[i], gen_np[i], data_range=2.0)\n",
    "            psnr_values.append(psnr)\n",
    "        return np.mean(psnr_values)\n",
    "    \n",
    "    def calculate_ssim(self, generated, target):\n",
    "        gen_np = generated.cpu().numpy()\n",
    "        target_np = target.cpu().numpy()\n",
    "        ssim_values = []\n",
    "        for i in range(gen_np.shape[0]):\n",
    "            gen_img = np.transpose(gen_np[i], (1, 2, 0))\n",
    "            target_img = np.transpose(target_np[i], (1, 2, 0))\n",
    "            ssim = structural_similarity(target_img, gen_img, data_range=2.0, channel_axis=2)\n",
    "            ssim_values.append(ssim)\n",
    "        return np.mean(ssim_values)\n",
    "    \n",
    "    def calculate_ciede2000(self, L, ab_generated, ab_target):\n",
    "        L_np = ((L.cpu().numpy() + 1.0) * 50.0)\n",
    "        ab_gen_np = ab_generated.cpu().numpy() * 128.0\n",
    "        ab_target_np = ab_target.cpu().numpy() * 128.0\n",
    "        \n",
    "        delta_e_values = []\n",
    "        for b in range(L_np.shape[0]):\n",
    "            L_channel = L_np[b, 0]\n",
    "            step = 8\n",
    "            sample_indices = [(i, j) for i in range(0, L_channel.shape[0], step) for j in range(0, L_channel.shape[1], step)]\n",
    "            \n",
    "            pixel_deltas = []\n",
    "            for i, j in sample_indices:\n",
    "                lab_gen = LabColor(L_channel[i, j], ab_gen_np[b, 0, i, j], ab_gen_np[b, 1, i, j])\n",
    "                lab_target = LabColor(L_channel[i, j], ab_target_np[b, 0, i, j], ab_target_np[b, 1, i, j])\n",
    "                delta_e = delta_e_cie2000(lab_gen, lab_target)\n",
    "                pixel_deltas.append(delta_e)\n",
    "            \n",
    "            delta_e_values.append(np.mean(pixel_deltas))\n",
    "        return np.mean(delta_e_values)\n",
    "    \n",
    "    def calculate_lpips(self, L, ab_generated, ab_target):\n",
    "        with torch.no_grad():\n",
    "            batch_size = L.size(0)\n",
    "            rgb_gen_list = []\n",
    "            rgb_target_list = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                L_np = ((L[i].cpu().squeeze().numpy() + 1.0) * 50.0)\n",
    "                ab_gen_np = ab_generated[i].cpu().permute(1, 2, 0).numpy() * 128.0\n",
    "                lab_gen = np.zeros((L_np.shape[0], L_np.shape[1], 3))\n",
    "                lab_gen[:, :, 0] = L_np\n",
    "                lab_gen[:, :, 1:] = ab_gen_np\n",
    "                rgb_gen = lab2rgb(lab_gen)\n",
    "                \n",
    "                ab_target_np = ab_target[i].cpu().permute(1, 2, 0).numpy() * 128.0\n",
    "                lab_target = np.zeros((L_np.shape[0], L_np.shape[1], 3))\n",
    "                lab_target[:, :, 0] = L_np\n",
    "                lab_target[:, :, 1:] = ab_target_np\n",
    "                rgb_target = lab2rgb(lab_target)\n",
    "                \n",
    "                rgb_gen_list.append(rgb_gen)\n",
    "                rgb_target_list.append(rgb_target)\n",
    "            \n",
    "            rgb_gen_tensor = torch.from_numpy(np.array(rgb_gen_list)).permute(0, 3, 1, 2).float() * 2 - 1\n",
    "            rgb_target_tensor = torch.from_numpy(np.array(rgb_target_list)).permute(0, 3, 1, 2).float() * 2 - 1\n",
    "            \n",
    "            rgb_gen_tensor = rgb_gen_tensor.to(self.device)\n",
    "            rgb_target_tensor = rgb_target_tensor.to(self.device)\n",
    "            \n",
    "            lpips_value = self.lpips_model(rgb_gen_tensor, rgb_target_tensor)\n",
    "            return lpips_value.mean().item()\n",
    "    \n",
    "    def evaluate_batch(self, L, ab_generated, ab_target):\n",
    "        metrics = {\n",
    "            'PSNR': self.calculate_psnr(ab_generated, ab_target),\n",
    "            'SSIM': self.calculate_ssim(ab_generated, ab_target),\n",
    "            'CIEDE2000': self.calculate_ciede2000(L, ab_generated, ab_target),\n",
    "            'LPIPS': self.calculate_lpips(L, ab_generated, ab_target)\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "evaluator = ColorimetricEvaluator(device)\n",
    "print(\"Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "generator.eval()\n",
    "\n",
    "metrics_sum = {'PSNR': 0.0, 'SSIM': 0.0, 'CIEDE2000': 0.0, 'LPIPS': 0.0}\n",
    "num_evaluated = 0\n",
    "\n",
    "print(f\"Evaluating Model {MODEL_ID} on test set...\")\n",
    "with torch.no_grad():\n",
    "    for L, ab_real, _ in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        L = L.to(device)\n",
    "        ab_real = ab_real.to(device)\n",
    "        \n",
    "        ab_fake = generator(L)\n",
    "        batch_metrics = evaluator.evaluate_batch(L, ab_fake, ab_real)\n",
    "        \n",
    "        for key in metrics_sum:\n",
    "            metrics_sum[key] += batch_metrics[key]\n",
    "        \n",
    "        num_evaluated += 1\n",
    "\n",
    "# Average metrics\n",
    "metrics_avg = {key: val / num_evaluated for key, val in metrics_sum.items()}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model {MODEL_ID} Evaluation Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"PSNR:        {metrics_avg['PSNR']:.4f} dB\")\n",
    "print(f\"SSIM:        {metrics_avg['SSIM']:.4f}\")\n",
    "print(f\"CIEDE2000:   {metrics_avg['CIEDE2000']:.4f}\")\n",
    "print(f\"LPIPS:       {metrics_avg['LPIPS']:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947231a",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "metrics_df = pd.DataFrame([{\n",
    "    'Model_ID': MODEL_ID,\n",
    "    'Architecture': 'U-Net',\n",
    "    'GAN': USE_GAN,\n",
    "    'L1_Loss': USE_L1,\n",
    "    'Perceptual_Loss': USE_PERCEPTUAL,\n",
    "    'Lambda_L1': LAMBDA_L1,\n",
    "    'Lambda_Perceptual': LAMBDA_PERCEPTUAL,\n",
    "    'PSNR': metrics_avg['PSNR'],\n",
    "    'SSIM': metrics_avg['SSIM'],\n",
    "    'CIEDE2000': metrics_avg['CIEDE2000'],\n",
    "    'LPIPS': metrics_avg['LPIPS'],\n",
    "    'Best_Val_Loss': best_val_loss,\n",
    "    'Epochs': EPOCHS\n",
    "}])\n",
    "\n",
    "csv_path = results_dir / f'metrics_{MODEL_ID}.csv'\n",
    "metrics_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Metrics saved to: {csv_path}\")\n",
    "\n",
    "# Save training history plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Generator loss with train and validation\n",
    "ax1.plot(history['epoch'], history['gen_loss'], 'b-', linewidth=2, label='Generator Loss (Train)', marker='o', markersize=4)\n",
    "ax1.plot(history['epoch'], history['val_loss'], 'r-', linewidth=2, label='Validation Loss', marker='s', markersize=4)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Generator Training History', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# Discriminator loss\n",
    "ax2.plot(history['epoch'], history['disc_loss'], 'g-', linewidth=2, label='Discriminator Loss', marker='^', markersize=4)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_title('Discriminator Training History', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.suptitle(f'Model {MODEL_ID} Training History', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = results_dir / f'training_history_{MODEL_ID}.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Training plot saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and save sample results\n",
    "generator.eval()\n",
    "num_samples = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    L, ab_real, gray_img = next(iter(test_loader))\n",
    "    L = L[:num_samples].to(device)\n",
    "    ab_real = ab_real[:num_samples]\n",
    "    gray_img = gray_img[:num_samples]\n",
    "    \n",
    "    ab_fake = generator(L).cpu()\n",
    "    L = L.cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Grayscale input\n",
    "        axes[i, 0].imshow(gray_img[i].squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title('Input (Grayscale)', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Generated colorization\n",
    "        L_np = ((L[i].squeeze().numpy() + 1.0) * 50.0)\n",
    "        ab_fake_np = ab_fake[i].permute(1, 2, 0).numpy() * 128.0\n",
    "        lab_fake = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "        lab_fake[:, :, 0] = L_np\n",
    "        lab_fake[:, :, 1:] = ab_fake_np\n",
    "        rgb_fake = lab2rgb(lab_fake)\n",
    "        \n",
    "        axes[i, 1].imshow(rgb_fake)\n",
    "        axes[i, 1].set_title('Generated Color', fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        ab_real_np = ab_real[i].permute(1, 2, 0).numpy() * 128.0\n",
    "        lab_real = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "        lab_real[:, :, 0] = L_np\n",
    "        lab_real[:, :, 1:] = ab_real_np\n",
    "        rgb_real = lab2rgb(lab_real)\n",
    "        \n",
    "        axes[i, 2].imshow(rgb_real)\n",
    "        axes[i, 2].set_title('Ground Truth', fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Model {MODEL_ID} - Colorization Results', fontsize=16, fontweight='bold', y=1.0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = results_dir / f'colorization_results_{MODEL_ID}.png'\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Visualization saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model {MODEL_ID} - Training and Evaluation Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Models saved in: {save_dir}\")\n",
    "print(f\"Results saved in: {results_dir}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
