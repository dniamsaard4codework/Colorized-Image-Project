{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f46955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lpips\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage.color import lab2rgb, rgb2lab\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from colormath.color_conversions import convert_color\n",
    "from colormath.color_diff import delta_e_cie2000\n",
    "from colormath.color_objects import LabColor, sRGBColor\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*negative Z values.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a488a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = torch.device(\"cuda\")\n",
    "        name = torch.cuda.get_device_name(0)\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        dev = torch.device(\"mps\")\n",
    "        name = \"Apple Silicon\"\n",
    "    else:\n",
    "        dev = torch.device(\"cpu\")\n",
    "        name = \"CPU\"\n",
    "    return dev, name\n",
    "\n",
    "device, device_name = get_device()\n",
    "print(f\"Using device: {device_name} ({device})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_ID = 'E'\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 256\n",
    "EPOCHS = 50\n",
    "SAVE_INTERVAL = 10\n",
    "\n",
    "# Model-specific configuration\n",
    "USE_GAN = False\n",
    "USE_L1 = True\n",
    "USE_PERCEPTUAL = True\n",
    "LAMBDA_L1 = 100.0\n",
    "LAMBDA_PERCEPTUAL = 10.0\n",
    "\n",
    "print(f\"Model {MODEL_ID} Configuration:\")\n",
    "print(f\"  GAN: {USE_GAN}\")\n",
    "print(f\"  L1 Loss: {USE_L1} (λ₁ = {LAMBDA_L1})\")\n",
    "print(f\"  Perceptual Loss: {USE_PERCEPTUAL} (λ₂ = {LAMBDA_PERCEPTUAL})\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ColorizeDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_size=256, split='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        \n",
    "        color_dir = self.root_dir / f\"{split}_color\"\n",
    "        self.color_paths = sorted(glob.glob(str(color_dir / \"*.jpg\")))\n",
    "        \n",
    "        black_dir = self.root_dir / f\"{split}_black\"\n",
    "        self.black_paths = sorted(glob.glob(str(black_dir / \"*.jpg\")))\n",
    "        \n",
    "        assert len(self.color_paths) == len(self.black_paths)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        print(f\"Loaded {len(self.color_paths)} {split} image pairs\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.color_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        color_img = Image.open(self.color_paths[idx]).convert('RGB')\n",
    "        color_img = self.transform(color_img)\n",
    "        \n",
    "        gray_img = Image.open(self.black_paths[idx]).convert('L')\n",
    "        gray_img = self.transform(gray_img)\n",
    "        \n",
    "        color_img_np = (color_img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "        lab_img = rgb2lab(color_img_np)\n",
    "        \n",
    "        L = lab_img[:, :, 0] / 50.0 - 1.0\n",
    "        ab = lab_img[:, :, 1:] / 128.0\n",
    "        \n",
    "        L = torch.from_numpy(L).unsqueeze(0).float()\n",
    "        ab = torch.from_numpy(ab).permute(2, 0, 1).float()\n",
    "        \n",
    "        return L, ab, gray_img\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ColorizeDataset('../data/colorize_dataset/data', img_size=IMG_SIZE, split='train')\n",
    "test_dataset = ColorizeDataset('../data/colorize_dataset/data', img_size=IMG_SIZE, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True if torch.cuda.is_available() else False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if torch.cuda.is_available() else False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd387ed",
   "metadata": {},
   "source": [
    "## U-Net Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f4403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net components\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down1 = nn.Conv2d(in_channels, 64, 4, 2, 1)\n",
    "        self.down2 = UNetBlock(64, 128, down=True)\n",
    "        self.down3 = UNetBlock(128, 256, down=True)\n",
    "        self.down4 = UNetBlock(256, 512, down=True)\n",
    "        self.down5 = UNetBlock(512, 512, down=True)\n",
    "        self.down6 = UNetBlock(512, 512, down=True)\n",
    "        self.down7 = UNetBlock(512, 512, down=True)\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
    "        self.up2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
    "        self.up3 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
    "        self.up4 = UNetBlock(1024, 512, down=False)\n",
    "        self.up5 = UNetBlock(1024, 256, down=False)\n",
    "        self.up6 = UNetBlock(512, 128, down=False)\n",
    "        self.up7 = UNetBlock(256, 64, down=False)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        \n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        \n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d7], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d6], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d5], dim=1))\n",
    "        u5 = self.up5(torch.cat([u4, d4], dim=1))\n",
    "        u6 = self.up6(torch.cat([u5, d3], dim=1))\n",
    "        u7 = self.up7(torch.cat([u6, d2], dim=1))\n",
    "        \n",
    "        output = self.final(torch.cat([u7, d1], dim=1))\n",
    "        return output\n",
    "\n",
    "# Initialize generator\n",
    "generator = UNetGenerator(in_channels=1, out_channels=2).to(device)\n",
    "print(\"U-Net Generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514c0ed",
   "metadata": {},
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptual Loss using VGG16\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual loss using VGG16 features\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            from torchvision.models import VGG16_Weights\n",
    "            vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
    "        except ImportError:\n",
    "            vgg = models.vgg16(pretrained=True).features\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.children())[:16]).eval()\n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.criterion = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, generated, target):\n",
    "        gen_rgb = torch.cat([generated, generated[:, :1, :, :]], dim=1)\n",
    "        target_rgb = torch.cat([target, target[:, :1, :, :]], dim=1)\n",
    "        \n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(generated.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(generated.device)\n",
    "        \n",
    "        gen_normalized = (gen_rgb - mean) / std\n",
    "        target_normalized = (target_rgb - mean) / std\n",
    "        \n",
    "        gen_features = self.feature_extractor(gen_normalized)\n",
    "        target_features = self.feature_extractor(target_normalized)\n",
    "        \n",
    "        return self.criterion(gen_features, target_features)\n",
    "\n",
    "perceptual_loss = PerceptualLoss().to(device)\n",
    "print(\"Perceptual loss initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbc2b2",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5154d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions and optimizer\n",
    "l1_loss = nn.L1Loss()\n",
    "optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': [], 'gen_loss': []}\n",
    "\n",
    "# Create save directory\n",
    "save_dir = Path(f'../models/{MODEL_ID}')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Model will be saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a56b2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04268c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(generator, train_loader, optimizer, epoch, device):\n",
    "    generator.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for L, ab_real, _ in pbar:\n",
    "        L = L.to(device)\n",
    "        ab_real = ab_real.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        ab_fake = generator(L)\n",
    "        \n",
    "        # Combined loss: L1 + Perceptual\n",
    "        loss_l1 = l1_loss(ab_fake, ab_real) * LAMBDA_L1\n",
    "        loss_perceptual = perceptual_loss(ab_fake, ab_real) * LAMBDA_PERCEPTUAL\n",
    "        loss = loss_l1 + loss_perceptual\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "# Train model\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    avg_loss = train_epoch(generator, train_loader, optimizer, epoch, device)\n",
    "    \n",
    "    history['train_loss'].append(avg_loss)\n",
    "    history['gen_loss'].append(avg_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'history': history,\n",
    "            'best_loss': best_loss\n",
    "        }\n",
    "        torch.save(checkpoint, save_dir / 'best_model.pt')\n",
    "        print(f\"✓ New best model saved! (Loss: {best_loss:.4f})\")\n",
    "    \n",
    "    # Save checkpoint at intervals\n",
    "    if epoch % SAVE_INTERVAL == 0:\n",
    "        checkpoint_path = save_dir / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"✓ Checkpoint saved at epoch {epoch}\")\n",
    "    \n",
    "    # Clear cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nTraining completed! Best loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f14e7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch numpy for colormath compatibility\n",
    "if not hasattr(np, 'asscalar'):\n",
    "    np.asscalar = lambda x: x.item() if hasattr(x, 'item') else float(x)\n",
    "\n",
    "# Evaluation metrics class\n",
    "class ColorimetricEvaluator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "        self.lpips_model.eval()\n",
    "    \n",
    "    def calculate_psnr(self, generated, target):\n",
    "        gen_np = generated.cpu().numpy()\n",
    "        target_np = target.cpu().numpy()\n",
    "        psnr_values = []\n",
    "        for i in range(gen_np.shape[0]):\n",
    "            psnr = peak_signal_noise_ratio(target_np[i], gen_np[i], data_range=2.0)\n",
    "            psnr_values.append(psnr)\n",
    "        return np.mean(psnr_values)\n",
    "    \n",
    "    def calculate_ssim(self, generated, target):\n",
    "        gen_np = generated.cpu().numpy()\n",
    "        target_np = target.cpu().numpy()\n",
    "        ssim_values = []\n",
    "        for i in range(gen_np.shape[0]):\n",
    "            gen_img = np.transpose(gen_np[i], (1, 2, 0))\n",
    "            target_img = np.transpose(target_np[i], (1, 2, 0))\n",
    "            ssim = structural_similarity(target_img, gen_img, data_range=2.0, channel_axis=2)\n",
    "            ssim_values.append(ssim)\n",
    "        return np.mean(ssim_values)\n",
    "    \n",
    "    def calculate_ciede2000(self, L, ab_generated, ab_target):\n",
    "        L_np = ((L.cpu().numpy() + 1.0) * 50.0)\n",
    "        ab_gen_np = ab_generated.cpu().numpy() * 128.0\n",
    "        ab_target_np = ab_target.cpu().numpy() * 128.0\n",
    "        \n",
    "        delta_e_values = []\n",
    "        for b in range(L_np.shape[0]):\n",
    "            L_channel = L_np[b, 0]\n",
    "            step = 8\n",
    "            sample_indices = [(i, j) for i in range(0, L_channel.shape[0], step) for j in range(0, L_channel.shape[1], step)]\n",
    "            \n",
    "            pixel_deltas = []\n",
    "            for i, j in sample_indices:\n",
    "                lab_gen = LabColor(L_channel[i, j], ab_gen_np[b, 0, i, j], ab_gen_np[b, 1, i, j])\n",
    "                lab_target = LabColor(L_channel[i, j], ab_target_np[b, 0, i, j], ab_target_np[b, 1, i, j])\n",
    "                delta_e = delta_e_cie2000(lab_gen, lab_target)\n",
    "                pixel_deltas.append(delta_e)\n",
    "            \n",
    "            delta_e_values.append(np.mean(pixel_deltas))\n",
    "        return np.mean(delta_e_values)\n",
    "    \n",
    "    def calculate_lpips(self, L, ab_generated, ab_target):\n",
    "        with torch.no_grad():\n",
    "            batch_size = L.size(0)\n",
    "            rgb_gen_list = []\n",
    "            rgb_target_list = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                L_np = ((L[i].cpu().squeeze().numpy() + 1.0) * 50.0)\n",
    "                ab_gen_np = ab_generated[i].cpu().permute(1, 2, 0).numpy() * 128.0\n",
    "                lab_gen = np.zeros((L_np.shape[0], L_np.shape[1], 3))\n",
    "                lab_gen[:, :, 0] = L_np\n",
    "                lab_gen[:, :, 1:] = ab_gen_np\n",
    "                rgb_gen = lab2rgb(lab_gen)\n",
    "                \n",
    "                ab_target_np = ab_target[i].cpu().permute(1, 2, 0).numpy() * 128.0\n",
    "                lab_target = np.zeros((L_np.shape[0], L_np.shape[1], 3))\n",
    "                lab_target[:, :, 0] = L_np\n",
    "                lab_target[:, :, 1:] = ab_target_np\n",
    "                rgb_target = lab2rgb(lab_target)\n",
    "                \n",
    "                rgb_gen_list.append(rgb_gen)\n",
    "                rgb_target_list.append(rgb_target)\n",
    "            \n",
    "            rgb_gen_tensor = torch.from_numpy(np.array(rgb_gen_list)).permute(0, 3, 1, 2).float() * 2 - 1\n",
    "            rgb_target_tensor = torch.from_numpy(np.array(rgb_target_list)).permute(0, 3, 1, 2).float() * 2 - 1\n",
    "            \n",
    "            rgb_gen_tensor = rgb_gen_tensor.to(self.device)\n",
    "            rgb_target_tensor = rgb_target_tensor.to(self.device)\n",
    "            \n",
    "            lpips_value = self.lpips_model(rgb_gen_tensor, rgb_target_tensor)\n",
    "            return lpips_value.mean().item()\n",
    "    \n",
    "    def evaluate_batch(self, L, ab_generated, ab_target):\n",
    "        metrics = {\n",
    "            'PSNR': self.calculate_psnr(ab_generated, ab_target),\n",
    "            'SSIM': self.calculate_ssim(ab_generated, ab_target),\n",
    "            'CIEDE2000': self.calculate_ciede2000(L, ab_generated, ab_target),\n",
    "            'LPIPS': self.calculate_lpips(L, ab_generated, ab_target)\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "evaluator = ColorimetricEvaluator(device)\n",
    "print(\"Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "generator.eval()\n",
    "\n",
    "metrics_sum = {'PSNR': 0.0, 'SSIM': 0.0, 'CIEDE2000': 0.0, 'LPIPS': 0.0}\n",
    "num_evaluated = 0\n",
    "\n",
    "print(f\"Evaluating Model {MODEL_ID} on test set...\")\n",
    "with torch.no_grad():\n",
    "    for L, ab_real, _ in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        L = L.to(device)\n",
    "        ab_real = ab_real.to(device)\n",
    "        \n",
    "        ab_fake = generator(L)\n",
    "        batch_metrics = evaluator.evaluate_batch(L, ab_fake, ab_real)\n",
    "        \n",
    "        for key in metrics_sum:\n",
    "            metrics_sum[key] += batch_metrics[key]\n",
    "        \n",
    "        num_evaluated += 1\n",
    "\n",
    "# Average metrics\n",
    "metrics_avg = {key: val / num_evaluated for key, val in metrics_sum.items()}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model {MODEL_ID} Evaluation Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"PSNR:        {metrics_avg['PSNR']:.4f} dB\")\n",
    "print(f\"SSIM:        {metrics_avg['SSIM']:.4f}\")\n",
    "print(f\"CIEDE2000:   {metrics_avg['CIEDE2000']:.4f}\")\n",
    "print(f\"LPIPS:       {metrics_avg['LPIPS']:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b623004",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_df = pd.DataFrame([{\n",
    "    'Model_ID': MODEL_ID,\n",
    "    'Architecture': 'U-Net',\n",
    "    'GAN': USE_GAN,\n",
    "    'L1_Loss': USE_L1,\n",
    "    'Perceptual_Loss': USE_PERCEPTUAL,\n",
    "    'Lambda_L1': LAMBDA_L1,\n",
    "    'Lambda_Perceptual': LAMBDA_PERCEPTUAL,\n",
    "    'PSNR': metrics_avg['PSNR'],\n",
    "    'SSIM': metrics_avg['SSIM'],\n",
    "    'CIEDE2000': metrics_avg['CIEDE2000'],\n",
    "    'LPIPS': metrics_avg['LPIPS'],\n",
    "    'Best_Loss': best_loss,\n",
    "    'Epochs': EPOCHS\n",
    "}])\n",
    "\n",
    "csv_path = results_dir / f'metrics_{MODEL_ID}.csv'\n",
    "metrics_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Metrics saved to: {csv_path}\")\n",
    "\n",
    "# Save training history plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['gen_loss'], 'b-', linewidth=2, label='Generator Loss')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title(f'Model {MODEL_ID} Training History', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = results_dir / f'training_history_{MODEL_ID}.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Training plot saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c03d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and save sample results\n",
    "generator.eval()\n",
    "num_samples = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    L, ab_real, gray_img = next(iter(test_loader))\n",
    "    L = L[:num_samples].to(device)\n",
    "    ab_real = ab_real[:num_samples]\n",
    "    gray_img = gray_img[:num_samples]\n",
    "    \n",
    "    ab_fake = generator(L).cpu()\n",
    "    L = L.cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Grayscale input\n",
    "        axes[i, 0].imshow(gray_img[i].squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title('Input (Grayscale)', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Generated colorization\n",
    "        L_np = ((L[i].squeeze().numpy() + 1.0) * 50.0)\n",
    "        ab_fake_np = ab_fake[i].permute(1, 2, 0).numpy() * 128.0\n",
    "        lab_fake = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "        lab_fake[:, :, 0] = L_np\n",
    "        lab_fake[:, :, 1:] = ab_fake_np\n",
    "        rgb_fake = lab2rgb(lab_fake)\n",
    "        \n",
    "        axes[i, 1].imshow(rgb_fake)\n",
    "        axes[i, 1].set_title('Generated Color', fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        ab_real_np = ab_real[i].permute(1, 2, 0).numpy() * 128.0\n",
    "        lab_real = np.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "        lab_real[:, :, 0] = L_np\n",
    "        lab_real[:, :, 1:] = ab_real_np\n",
    "        rgb_real = lab2rgb(lab_real)\n",
    "        \n",
    "        axes[i, 2].imshow(rgb_real)\n",
    "        axes[i, 2].set_title('Ground Truth', fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Model {MODEL_ID} - Colorization Results', fontsize=16, fontweight='bold', y=1.0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = results_dir / f'colorization_results_{MODEL_ID}.png'\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Visualization saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model {MODEL_ID} - Training and Evaluation Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Models saved in: {save_dir}\")\n",
    "print(f\"Results saved in: {results_dir}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
